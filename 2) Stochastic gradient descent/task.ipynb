{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана следующая задача: $(x−μ)^⊤A(x−μ) \\to min$\n",
    "\n",
    "При ограничении Евклидовой нормой: $∥x∥^2_2=∥x∥^2≤1$, что эквивалентно $(\\sqrt{\\sum_{i=1}^{n}|x_i|^2})^2=\\sum_{i=1}^{n}x_i^2=x_1^2+...+x_n^2≤1$\n",
    "\n",
    "Где: где $x∈R^n$, $A$ - симметричная, положительно определенная матрица, $μ = (1, 1, . . . , 1)^⊤ ∈ R^n$.\n",
    "1) Записать ***двойственную к ней задачу*** и применить метод из Лабораторной работы #1 к двойственной задаче.\n",
    "2) Для метода из Лабораторной работы #1 замените градиент на ***стохастический градиент*** (шаг нужно выбирать тем же способом, как и в обычном градиентном спуске): <br>\n",
    "    $\\widetilde{∇}f(x)=\\frac{1}{m}\\sum_{j=1}^{m}(0, 0, ..., 0, nf'_{x_i}, 0, ..., 0)^T$, где $i$ выбирается случайным образом от 1 до n.<br>\n",
    "И применить метод стохастического градиентного спуска ***для различных значений параметра*** $m ∈ \\{1, \\frac{n}{8} , \\frac{n}{4} , \\frac{n}{2} , n\\}$.\n",
    "3) Представить следующие результаты:\n",
    "    * Для заданной точности $ε > 0$ **по значению целевой функции** и для каждого значения размерности $n ∈ \\{10, 20, ..., 100\\}$ и $m ∈ \\{1, \\frac{n}{8} , \\frac{n}{4} , \\frac{n}{2} , n\\}$ подсчитайте ***среднее число арифметических операций*** для обычного и стохастического градиентного спуска в прямой задаче и в двойственной (стохастический метод для двойственной задачи в данном случае аналогичен прямому методу), усреднение проводится по всем начальным точкам P и по всем тестовым примерам N. Если число операций подсчитать не удается, то указать среднее время работы метода. Какой метод работает лучше и почему?\n",
    "    * Для одного тестового примера при $n = 10$ и нескольких различных начальных точек постройте ***зависимость точности от числа арифметических операций*** в методах градиентного и стохастического градиентного спуска, примененных для прямой задачи. Если число операций подсчитать не получается, то построить зависимость средней точности от времени работы метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройки/Импорты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Версии важных модулей:\n",
    "* cvxpy==1.4.3\n",
    "* numpy==1.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp # солвер для задач\n",
    "import numpy as np # для работы с массивами\n",
    "from sklearn.datasets import make_spd_matrix # для генерации симметричной положительно определённой матрицы A для поставленной задачи\n",
    "\n",
    "import time # для отслеживания времени выполнения\n",
    "from tqdm import tqdm # для отслеживания прогресса\n",
    "from matplotlib import pyplot as plt # для построения графиков/вывода изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(10, 101, 10) # возможные значения n (число переменных в задаче ~ размерность пространства) от 10 до 100 включительно\n",
    "N = 50 # число тестовых примеров для каждого значения n\n",
    "P = 50 # число начальных точек для каждого примера N\n",
    "ε = 0.01 # необходимая точность\n",
    "\n",
    "DIM = 10 # интересующая нас размерность пространства, на которой будут проходить тесты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_primal(x: np.array, A: np.array, μ: np.array) -> np.float32:\n",
    "    \"\"\"\n",
    "    Прямая функция из задачи.\\n\n",
    "    Parameters:\n",
    "        * x: текущие значения x (в виде столбца)\n",
    "        * A: матрица A\n",
    "        * μ: смещение центра функции (в виде столбца)\\n\n",
    "    Returns:\n",
    "        * np.float32: значение функции в точке x\n",
    "    \"\"\"\n",
    "    return ((x-μ).T @ A @ (x-μ))[0] # значение функции задачи (было бы (x-μ) @ A @ (x-μ).reshape((dim,1)), если бы x шёл как строка, а не столбец) ([0] — из-за вложенности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_primal_grad(x: np.array, A: np.array, μ: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Производная прямой функции из задачи.\\n\n",
    "    Parameters:\n",
    "        * x: текущие значения x (в виде вектора-столбца)\n",
    "        * A: матрица A\n",
    "        * μ: смещение центра функции (в виде вектора-столбца)\\n\n",
    "    Returns:\n",
    "        * np.array: вектор-столбец градиента функции в точке x\n",
    "    \"\"\"\n",
    "    return 2 * A @ (x - μ) # значение производной функции задачи\n",
    "    # return 2 * A @ x # значение производной функции задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraints_primal(x: np.array) -> bool:\n",
    "    \"\"\"\n",
    "    Функция для проверка решения прямой задачи на допустимость.\\n\n",
    "    Parameters:\n",
    "        * x: текущие значения x (в виде вектора-столбца)\\n\n",
    "    Returns:\n",
    "        * bool: True — если решение допустимо, иначе — False\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x, ord=2) ** 2 <= 1 # квадрат евклидовой нормы решения должен быть ≤ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraints_primal_grad(x: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Производная функции ограничения из задачи.\\n\n",
    "    Parameters:\n",
    "        * x: текущие значения x (в виде вектора-столбца)\\n\n",
    "    Returns:\n",
    "        * np.array: значение производной функции ограничения в точке x\n",
    "    \"\"\"\n",
    "    return 2 * x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal(x: np.array, A: np.array, μ: np.array) -> bool:\n",
    "    \"\"\"\n",
    "    Функция для проверки оптимальности решения прямой задачи. Необходимо, чтобы вектора градиента к ограничению и антиградиента к целевой функции были сонаправлены. \n",
    "    Для проверки сонаправленности необходимо, чтобы вектора были коллинеарны (x1/y1 = x2/y2 = ... = xn/yn) и их скалярное произведение (dot product) было положительным.\\n\n",
    "    Parameters:\n",
    "        * x: текущие значения x (в виде вектора-столбца)\n",
    "        * A: матрица A\n",
    "        * μ: смещение центра функции (в виде вектора-столбца)\\n\n",
    "    Returns:\n",
    "        * bool: True — если решение оптимально, False — иначе\n",
    "    \"\"\"\n",
    "    grad_const = constraints_primal_grad(x) # значение вектора градиента ограничений\n",
    "    grad_func = func_primal_grad(x, A, μ) # значение вектора градиента функции\n",
    "\n",
    "    # relation = grad_const / grad_func # отношение (x1/y1, x2/y2, ..., xn/yn) для проверки коллинеарности \n",
    "    # assert np.isclose(relation, relation[0], atol=0.1).all(), \"Вектора не коллинеарны!\" # если не все значения (x1/y1, x2/y2, ..., xn/yn) совпадают с точностью до atol — выкидываем ошибку\n",
    "\n",
    "    return grad_const.T @ -grad_func > 0 # проверка сонаправленнсоти (скалярное произведение вектора градиента к ограничению и антиградиента к целевой функции положительно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x_1: np.array, x_2: np.array) -> np.float32:\n",
    "    \"\"\"Функция для подсчёта Евклидового расстояния между точками.\\n\n",
    "    Parameters:\n",
    "        * x_1: от какой точки считать (значения в виде вектора-столбца)\n",
    "        * x_2: до какой точки считать (значения в виде вектора-столбца)\\n\n",
    "    Returns:\n",
    "        * np.float32: дистанция до точки\n",
    "    \"\"\"\n",
    "    # return np.linalg.norm(x_2 - x_1, ord=2) # считаем Евклидово расстояние\n",
    "    return ((x_2 - x_1)**2).sum()**(1/2) # считаем Евклидово расстояние (корень из суммы квадратов разности координат точек)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Двойственная задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение двойственной задачи для ***линейной функции с линейными ограничениями*** (линейное программирование).\n",
    "\n",
    "Если прямая задача определена как:\n",
    "* Дан набор из ***n*** переменных: $x_1, ..., x_n$.\n",
    "* Для каждой переменной $x_i$ определено ограничение на знак — она должна быть либо неотрицательной $(x_i ≥ 0)$, либо неположительной $(x_i ≤ 0)$, либо ограничение не задано $(x_i ∈ R)$.\n",
    "* Задана целевая функция: $c_1 x_1+...+c_n x_n \\to max$.\n",
    "* Задан список из ***m*** ограничений. Каждое ограничение *j* равно: $a_{j1} x_1+...+a_{jn} x_n\\ ?\\ b_j$, где на месте знака $?$ может быть $≥$, $=$, $≤$.\n",
    "\n",
    "Тогда двойственная для неё будет строиться следующим образом:\n",
    "* Каждое ограничение прямой задачи становится двойственной переменной. Таким образом, получаем ***m*** переменных: $y_1, ..., y_m$.\n",
    "* Знак ограничения каждой двойственной переменной «противоположен» знаку ограничения в прямой задаче. Таким образом: \n",
    "    * $≥ b_j$ $\\to$ $y_j ≤ 0$\n",
    "    * $≤ b_j$ $\\to$ $y_j ≥ 0$\n",
    "    * $= b_j$ $\\to$ $y_j ∈ R$\n",
    "* Целевая функция двойственной задачи равна: $b_1 y_1+...+b_n y_n \\to min$.\n",
    "* Каждая переменная прямой задачи становится двойственным ограничением. Таким образом, получаем ***n*** ограничений. Коэффициент двойственной переменной в двойственных ограничениях равен коэффициенту переменной из ограничения прямой задачи. Таким образом, каждое ограничение *i* есть: $a_{1i} y_1 +...+ a_{mi} y_m\\ ?\\ c_i$, где знак $?$ аналогичен ограничению на переменную $x_i$ в прямой задаче:\n",
    "    * $x_i ≤ 0$ $\\to$ $≤ c_i$\n",
    "    * $x_i ≥ 0$ $\\to$ $≥ c_i$\n",
    "    * $x_i ∈ R$ $\\to$ $= c_i$\n",
    "\n",
    "Если все ограничения имеют один и тот же знак, тогда прямую и двойственную задачу можно представить в виде векторных формулировок:\n",
    "\n",
    "| Прямая | Двойственная |\n",
    "| --- | --- |\n",
    "| Целевая функция: $c^Tx \\to max$ <br> Ограничения: $Ax ≤ b$, $x ≥ 0$ | Целевая функция: $b^Ty \\to min$ <br> Ограничения: $A^Ty ≥ c$, $y ≥ 0$ |\n",
    "| Целевая функция: $c^Tx \\to max$ <br> Ограничения: $Ax ≤ b$ | Целевая функция: $b^Ty \\to min$ <br> Ограничения: $A^Ty = c$, $y ≥ 0$ |\n",
    "| Целевая функция: $c^Tx \\to max$ <br> Ограничения: $Ax = b$, $x ≥ 0$ | Целевая функция: $b^Ty \\to min$ <br> Ограничения: $A^Ty ≥ c$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Однако наша задача — ***квадратичная с квадратичными ограничениями***, из-за чего двойственную к ней нужно строить иным образом.\n",
    "Если прямая задача определена как:\n",
    "* Дан набор из ***n*** переменных (вектор-столбец): $x = (x_1, ..., x_n)^T = \\begin{pmatrix} x_1 \\\\ ... \\\\ x_n \\end{pmatrix}$.\n",
    "* Задана целевая функция (выпуклая): $x^T A x + c^T x \\to min$.\n",
    "* Задано ограничение (выпуклое): $x^T x ≤ b$.\n",
    "\n",
    "Тогда двойственная для неё будет строиться следующим образом (с помощью функции Лагранжа):\n",
    "* Объединяем целевую функцию прямой задачи с её ограничениями (умноженными на коэффициент $λ$), получив тем самым функцию Лагранжа: \n",
    "    * $L(x, λ) = x^T A x + c^T x + λ(x^T x - b)$\n",
    "* Если целевая функция и её ограничения — выпуклы, тогда седловая точка $(x^*, λ^*)$ Лагранжиана есть оптимум для прямой $f(x)$ и двойственной $g(λ)$ задачи.\n",
    "* Минимизируем Лагранжиан $L(x, λ)$ по $x$, для этого приравняв к нулю его производную по $x$ (производные по $x_i$ в случае не матричной записи):\n",
    "    * $\\frac{dL(x, λ)}{dx} = 2Ax + c^T + 2λx = 0$\n",
    "* Выражаем из $\\frac{dL(x, λ)}{dx} = 0$ оптимальное значение $x^*$:\n",
    "    * $2Ax^* + c^T + 2λx^* = 0$\n",
    "    * $2Ax^* + 2λx^* = -c^T$\n",
    "    * $2(Ax^* + λx^*) = -c^T$\n",
    "    * $Ax^* + λx^* = -\\frac{1}{2}c^T$\n",
    "    * $(A + λE)x^* = -\\frac{1}{2}c^T$, где матрица $E$ — единичная матрица, у которой элементы на диагонали — 1, а всё остальное — 0. \n",
    "    * $x^* = -\\frac{1}{2}(A + λE)^{-1}c^T$.\n",
    "* Подставляем полученное $x^*$ на место $x$ в Лагранжиан $L(x, λ)$, тем самым получив ***двойственную функцию $g(λ)$***:\n",
    "    * $g(λ) = (-\\frac{1}{2}(A + λE)^{-1}c^T)^TA(-\\frac{1}{2}(A + λE)^{-1}c^T) + c^T (-\\frac{1}{2}(A + λE)^{-1}) -λb$\n",
    "* Таким образом двойственная задача выражается из Лагранжевой функции:\n",
    "    * Целевая функция: $g(λ) = (-\\frac{1}{2}(A + λE)^{-1}c^T)^TA(-\\frac{1}{2}(A + λE)^{-1}c^T) + c^T (-\\frac{1}{2}(A + λE)^{-1}) -λb \\to max$\n",
    "    * Ограничение: $λ ≥ 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В конкретно нашей задаче \n",
    "Целевая функция имеет вид: $(x−μ)^⊤A(x−μ) \\to min$. <br>\n",
    "А ограничение: $∥x∥^2_2 = ∥x∥^2 = x^Tx ≤1$.\n",
    "\n",
    "Тогда её функция Лагранжа $L(x, λ)$ имеет вид: \n",
    "* $L(x, λ) = (x−μ)^⊤A(x−μ) + λ(x^T x - 1)$.\n",
    "\n",
    "Производная функции Лагранжа $\\frac{dL(x, λ)}{dx}$:\n",
    "* $\\frac{dL(x, λ)}{dx} = 2A(x−μ) + 2λx = 0$, сократим её в два раза (ничего не потеряем, так как всё равно приравниваем к нулю) $\\frac{dL(x, λ)}{dx} = A(x−μ) + λx = 0$.\n",
    "\n",
    "Выражаем значение $x^*$:\n",
    "* $A(x^*−μ) + λx^* = 0$\n",
    "* $Ax^* − Aμ + λx^* = 0$\n",
    "* $Ax^* + λx^* = Aμ$\n",
    "* $(A + λE)x^* = Aμ$\n",
    "* $x^* = (A + λE)^{-1}Aμ$\n",
    "\n",
    "Подставляем полученное значение $x^*$ в $L(x, λ)$:\n",
    "* $g(λ) = ((A + λE)^{-1}Aμ-μ)^T A ((A + λE)^{-1}Aμ-μ) + λ(((A + λE)^{-1}Aμ)^T (A + λE)^{-1}Aμ - 1)$\n",
    "\n",
    "Таким образом ***двойственная задача*** выглядит следующим образом:\n",
    "* Целевая функция: $g(λ) = ((A + λE)^{-1}Aμ-μ)^T A ((A + λE)^{-1}Aμ-μ) + λ(((A + λE)^{-1}Aμ)^T (A + λE)^{-1}Aμ - 1) \\to max$\n",
    "* Ограничение: $λ ≥ 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда производная двойственной функции $g(λ)$ по $λ$ имеет следующий вид:\n",
    "* $∇g(λ) = \\frac{dg(λ)}{dλ} = -2((A+λE)^{-1}Aμ - μ)^T A ((A+λE)^{-2}Aμ) + (((A+λE)^{-1}Aμ)^T (A+λE)^{-1}Aμ - 1) - 2λ((A+λE)^{-1}Aμ)^T (A+λE)^{-2}Aμ$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упростим двойственную целевую функцию и её производную с помощью замены $t_1 = (A+λE)^{-1}$ и $t_2 = Aμ$, тогда производная будет выглядеть:\n",
    "* $g(λ) = (t_1 t_2 - μ)^T A (t_1 t_2 - μ) + λ((t_1 t_2)^T t_1 t_2 - 1) \\to max$\n",
    "* $∇g(λ) = -2(t_1 t_2 - μ)^T A (t_1^2 t_2) + ((t_1 t_2)^T t_1 t_2 - 1) -2λ(t_1 t_2)^T t_1^2 t_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_dual(λ: np.array, A: np.array, μ: np.array) -> np.float32:\n",
    "    \"\"\"\n",
    "    Двойственная функция (задача) для данной квадратичной задачи с квадратичными ограничениями (построена с использованием функции Лагранжа).\\n\n",
    "    Parameters:\n",
    "        * λ: текущие значения λ\n",
    "        * A: матрица A\n",
    "        * μ: смещение центра функции (в виде столбца)\\n\n",
    "    Returns:\n",
    "        * np.float32: значение двойственной функции в точке λ\n",
    "    \"\"\"\n",
    "    n = A.shape[0] # размерность матрицы A\n",
    "    t_1 = np.linalg.inv(A + λ * np.eye(n)) # значение первого сокращения (np.eye(n) — единичная матрица размера n)\n",
    "    t_2 = A @ μ # значение второго сокращения\n",
    "    return ((t_1 @ t_2 - μ).T @ A @ (t_1 @ t_2 - μ) + λ * ((t_1 @ t_2).T @ t_1 @ t_2 - 1))[0] # значение двойственной функции ([0] — из-за вложенности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_dual_grad(λ: np.array, A: np.array, μ: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Производная двойственной функции из задачи.\\n\n",
    "    Parameters:\n",
    "        * λ: текущие значения λ\n",
    "        * A: матрица A\n",
    "        * μ: смещение центра функции (в виде вектора-столбца)\\n\n",
    "    Returns:\n",
    "        * np.array: вектор-столбец градиента функции в точке x\n",
    "    \"\"\"\n",
    "    n = A.shape[0] # размерность матрицы A\n",
    "    t_1 = np.linalg.inv(A + λ * np.eye(n)) # значение первого сокращения (np.eye(n) — единичная матрица размера n)\n",
    "    t_2 = A @ μ # значение второго сокращения\n",
    "    return -2 * (t_1 @ t_2 - μ).T @ A @ (t_1 @ t_1 @ t_2) + ((t_1 @ t_2).T @ t_1 @ t_2 - 1) -2 * λ * (t_1 @ t_2).T @ t_1 @ t_1 @ t_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraints_dual(λ: np.array) -> bool:\n",
    "    \"\"\"\n",
    "    Функция для проверка решения двойственной задачи на допустимость.\\n\n",
    "    Parameters:\n",
    "        * λ: текущие значения λ\\n\n",
    "    Returns:\n",
    "        * bool: True — если решение допустимо, иначе — False\n",
    "    \"\"\"\n",
    "    return λ >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем истинные ответы от солвера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "data = {} # словарь под данные для теста\n",
    "\n",
    "for dim in tqdm(n): # идём по возможному числу переменных (размерности пространства)\n",
    "    data[dim] = {i: {} for i in range(N)} # подсловарь под тест-кейсы для рассматриваемой размерности dim (получилась тройная вложенность словаря)\n",
    "    for i in range(N): # идём по числу тест-кейсов\n",
    "        # решаем прямую задачу с помощью солвера\n",
    "        A = make_spd_matrix(n_dim=dim) # генерируем случайную симметричную положительно определённую матрицу A\n",
    "        x = cp.Variable(shape=(dim, 1)) # значения переменных \n",
    "        μ = np.ones(shape=(dim, 1)) # смещение центра функции\n",
    "        objective = cp.Minimize(cp.quad_form(x-μ, A)) # минимизируем квадратичную функцию\n",
    "        # objective = cp.Minimize((x-μ).T @ A @ (x-μ)) # как должно быть с обычной математикой (не запустить, так как CVX считает такую проблему не выпуклой)\n",
    "        constraints = [cp.sum_squares(x) <= 1] # накладываемое ограничение — сумма квадратов переменных меньше или равна 1\n",
    "        problem = cp.Problem(objective, constraints) # создаём объект решаемой задачи\n",
    "        res = problem.solve(solver=cp.ECOS) # решаем поставленную проблему с помощью solver\n",
    "\n",
    "        optimal(x.value, A, μ) # проверяем решение на оптимальность\n",
    "\n",
    "        data[dim][i][\"A\"] = A # запоминаем матрицу A\n",
    "        data[dim][i][\"X opt solver\"] = x.value # оптимальное значение X от встроенного солвера\n",
    "        data[dim][i][\"Result solver\"] = res # ответ от встроенного солвера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация градиентного спуска для сильно выпуклых и гладких функций (Gradient descent for α-strongly convex and β-smooth functions with projection) в прямой задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_primal(x: np.array, A: np.array, res_solver: np.float32, ε: np.float32) -> list:\n",
    "    \"\"\"\n",
    "    Метод градиентного спуска для подсчёта оптимума прямой задачи.\\n\n",
    "    Parameters:\n",
    "        * x: изначальное значение x\n",
    "        * A: матрица A\n",
    "        * res_solver: уже полученный ответ от солвера, к которому нужно сойтись\n",
    "        * ε: необходимая точность ответа\\n\n",
    "    Returns:\n",
    "        * list: [оптимальное значение функции, оптимальная точка x, число итераций]\n",
    "    \"\"\"\n",
    "    iterations = 0 # счётчик итераций градиентного спуска\n",
    "\n",
    "    # считаем шаг для метода\n",
    "    eigenvalues = np.linalg.eigvals(A) # собственные значения матрицы A\n",
    "    α = 2 * eigenvalues.min() # параметр выпуклости функции, в данном случае равен 2 * минимальное собственное значение матрицы A\n",
    "    β = 2 * eigenvalues.max() # параметр гладкости функции, в данном случае равен 2 * максимальное собственное значение матрицы A\n",
    "    η = 2 / (α + β) # значение шага\n",
    "\n",
    "    dim = x.shape[0] # размерность пространства\n",
    "    μ = np.ones(shape=(dim, 1)) # смещение центра функции, (dim, 1) — в виде столбца\n",
    "    constraint_center = np.zeros(shape=(dim, 1)) # координаты центра сферы-ограничения задачи\n",
    "\n",
    "    res_grad_primal = func_primal(x, A, μ) # # значение начального решения для рассматриваемой стартовой точки\n",
    "    while abs(res_solver - res_grad_primal) > ε: # пока не сошлись с ответом солвера\n",
    "        x = x - η * func_primal_grad(x, A, μ) # обновляем значение x (так как задача минимизации, то идём против градиента)\n",
    "        if dist(constraint_center, x) > 1: # если нарушили ограничение ~ вышли из допустимой области (Евклидово расстояние от нуля до точки должно быть <= 1)\n",
    "            x = x / dist(constraint_center, x) # берём проекцию точки на dim-мерную сферу (делим координаты на расстояние до точки, чтобы новое расстояние было равно 1)\n",
    "\n",
    "        res_grad_primal = func_primal(x, A, μ) # считаем значение функции\n",
    "        \n",
    "        iterations += 1 # увеличиваем общее число итераций на рассматриваемой размерности dim\n",
    "\n",
    "    return [res_grad_primal, x, iterations] # возвращаем [оптимальное значение функции, оптимальная точка x, число итераций]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for dim in tqdm(n): # идём по возможному числу переменных (размерности пространства)\n",
    "    iterations = 0 # всего итераций для решения всех тест-кейсов при всех начальных точках\n",
    "    time_start = time.time() # замеряем время старта рассмотрения размерности dim\n",
    "\n",
    "    for i in range(N): # идём по числу тест-кейсов\n",
    "        A = data[dim][i][\"A\"] # матрица А для тест-кейса\n",
    "        res_solver = data[dim][i][\"Result solver\"] # результат от солвера для тест-кейса\n",
    "        \n",
    "        for p in range(P): # идём по числу случайных стартовых точек\n",
    "            # генерируем начальную точку при ограничение задачи — квадрат евклидовой нормы вектора x ≤ 1\n",
    "            x = np.random.randint(low=0, high=100, size=(dim, 1)) # генерируем случайные числа размера количества переменных (изначально они могут нарушать ограничения)\n",
    "            x = x/np.linalg.norm(x, ord=2) # делим значение сгенерированного вектора на его евклидову норму, чтобы новая норма была точно ≤ 1 (на самом деле она будет лишь слегка меньше 1, но такая точка уже не нарушает ограничение задачи)\n",
    "\n",
    "            iterations += gradient_descent_primal(x, A, res_solver, ε)[2] # запоминаем число итераций, что потребовалось градиентному спуску чтобы сойтись с ответом солвера с точностью ε\n",
    "\n",
    "    data[dim][\"Average time grad prime\"] = (time.time() - time_start) / (N * P) # среднее время для размерности dim за (N * p) решённых вариантов задачи\n",
    "    data[dim][\"Average iterations grad prime\"] = iterations / (N * P) # среднее число итерации для размерности dim за (N * p) решённых вариантов задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация градиентного спуска для сильно выпуклых и гладких функций (Gradient descent for α-strongly convex and β-smooth functions with projection) в двойственной задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_dual(λ: np.array, A: np.array, res_solver: np.float32, ε: np.float32) -> list:\n",
    "    \"\"\"\n",
    "    Метод градиентного спуска для подсчёта оптимума двойственной задачи.\\n\n",
    "    Parameters:\n",
    "        * λ: изначальное значения λ\n",
    "        * A: матрица A\n",
    "        * res_solver: уже полученный ответ от солвера, к которому нужно сойтись\n",
    "        * ε: необходимая точность ответа\\n\n",
    "    Returns:\n",
    "        * list: [оптимальное значение функции, оптимальное значение λ, число итераций]\n",
    "    \"\"\"\n",
    "    iterations = 0 # счётчик итераций градиентного спуска\n",
    "\n",
    "    # считаем шаг для метода\n",
    "    eigenvalues = np.linalg.eigvals(A) # собственные значения матрицы A\n",
    "    α = 2 * eigenvalues.min() # параметр выпуклости функции, в данном случае равен 2 * минимальное собственное значение матрицы A\n",
    "    β = 2 * eigenvalues.max() # параметр гладкости функции, в данном случае равен 2 * максимальное собственное значение матрицы A\n",
    "    η = 2 / (α + β) # значение шага\n",
    "\n",
    "    dim = A.shape[1] # размерность пространства\n",
    "    μ = np.ones(shape=(dim, 1)) # смещение центра функции, (dim, 1) — в виде столбца\n",
    "\n",
    "    res_grad_dual = func_dual(λ, A, μ) # # значение начального решения для рассматриваемой стартовой точки\n",
    "    while abs(res_solver - res_grad_dual) > ε: # пока не сошлись с ответом солвера\n",
    "        λ = λ + η * func_dual_grad(λ, A, μ) # обновляем значение λ (так как задача максимизации, то идём в сторону градиента)\n",
    "        if not constraints_dual(λ): # если нарушено ограничение на λ\n",
    "            λ = 0\n",
    "\n",
    "        res_grad_dual = func_dual(λ, A, μ) # считаем значение функции\n",
    "        \n",
    "        iterations += 1 # увеличиваем общее число итераций на рассматриваемой размерности dim\n",
    "\n",
    "    return [res_grad_dual, λ, iterations] # возвращаем [оптимальное значение функции, оптимальное значение λ, число итераций]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.11it/s]\n",
      "100%|██████████| 50/50 [00:04<00:00, 11.11it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 16.71it/s]\n",
      "\n",
      " 76%|███████▌  | 38/50 [00:02<00:00, 18.93it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for dim in tqdm(n): # идём по возможному числу переменных (размерности пространства)\n",
    "    iterations = 0 # всего итераций для решения всех тест-кейсов при всех начальных точках\n",
    "    time_start = time.time() # замеряем время старта рассмотрения размерности dim\n",
    "\n",
    "    for i in range(N): # идём по числу тест-кейсов\n",
    "        A = data[dim][i][\"A\"] # матрица А для тест-кейса\n",
    "        res_solver = data[dim][i][\"Result solver\"] # результат от солвера для тест-кейса\n",
    "        \n",
    "        for p in range(P): # идём по числу случайных стартовых точек\n",
    "            λ = np.random.rand(1, 1) # генерируем случайное значение λ (np.array двойной вложенности) из равномерного распределения [0, 1), удоавлетворяющее ограничению λ ≥ 0\n",
    "            \n",
    "            iterations += gradient_descent_dual(λ, A, res_solver, ε)[2] # запоминаем число итераций, что потребовалось градиентному спуску чтобы сойтись с ответом солвера с точностью ε\n",
    "\n",
    "    data[dim][\"Average time grad dual\"] = (time.time() - time_start) / (N * P) # среднее время для размерности dim за (N * p) решённых вариантов задачи\n",
    "    data[dim][\"Average iterations grad dual\"] = iterations / (N * P) # среднее число итерации для размерности dim за (N * p) решённых вариантов задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Реализация, сравнение и тестирование стохастического градиентного спуска (Stochastic gradient descent) с градиентным спуском из первой лабораторной работы (Gradient descent for α-strongly convex and β-smooth functions with projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастический градиент: $\\widetilde{∇}f(x)=\\frac{1}{m}\\sum_{j=1}^{m}(0, 0, ..., 0, nf'_{x_i}, 0, ..., 0)^T$, где $i$ выбирается случайным образом от 1 до $n$ на каждой итерации $j$, $m$ — варьируемый параметр. <br>\n",
    "Основным отличием стохастического градиентного спуска от его обычной версии является то, что шаг оптимизации происходит по произвольной компоненте, а не по всем сразу. Благодаря этому можно получить ускорение за счёт подсчёта производной не по всем компонентам, но ценой данного действия будет точность и скорость сходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При оставлении шага из первой лабораторной работы $η = \\frac{2}{α + β}$ неизменным <span style=\"color:red\"> стохастический градиент может не сходиться</span>, поэтому на каждой итерации будем слегка уменьшать его значение (простым умножением на 0.99, но не менее 0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация стохастического градиентного спуска для сильно выпуклых и гладких функций (Gradient descent for α-strongly convex and β-smooth functions with projection) в прямой задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_primal(x: np.array, A: np.array, res_solver: np.float32, ε: np.float32, m: np.float32) -> list:\n",
    "    \"\"\"\n",
    "    Метод стохастического градиентного спуска для подсчёта оптимума прямой задачи.\\n\n",
    "    Parameters:\n",
    "        * x: изначальное значение x\n",
    "        * A: матрица A\n",
    "        * res_solver: уже полученный ответ от солвера, к которому нужно сойтись\n",
    "        * ε: необходимая точность ответа\n",
    "        * m: параметр метода стохастического градиентного спуска, сколько аккумулировать градиентов\\n\n",
    "    Returns:\n",
    "        * list: [оптимальное значение функции, оптимальная точка x, число итераций]\n",
    "    \"\"\"\n",
    "    iterations = 0 # счётчик итераций стохастического градиентного спуска\n",
    "\n",
    "    # считаем шаг для метода\n",
    "    eigenvalues = np.linalg.eigvals(A) # собственные значения матрицы A\n",
    "    α = 2 * eigenvalues.min() # параметр выпуклости функции, в данном случае равен 2 * минимальное собственное значение матрицы A\n",
    "    β = 2 * eigenvalues.max() # параметр гладкости функции, в данном случае равен 2 * максимальное собственное значение матрицы A\n",
    "    η = 2 / (α + β) # значение шага\n",
    "\n",
    "    dim = x.shape[0] # размерность пространства\n",
    "    μ = np.ones(shape=(dim, 1)) # смещение центра функции, (dim, 1) — в виде столбца\n",
    "    constraint_center = np.zeros(shape=(dim, 1)) # координаты центра сферы-ограничения задачи\n",
    "\n",
    "    res_stoh_grad_primal = func_primal(x, A, μ) # значение начального решения для рассматриваемой стартовой точки\n",
    "    while abs(res_solver - res_stoh_grad_primal) > ε: # пока не сошлись с ответом солвера\n",
    "        #========================= реализация через простую итерацию =================================\n",
    "        grad = np.zeros(shape=(dim, 1)) # \"накопительный\" вектор для градиента по компонентам\n",
    "        for j in range(m): # делаем m выборов компоненты, по которой будет делать градиентный шаг\n",
    "            i = np.random.randint(low=0, high=dim, size=(1)) # случайным образом выбираем компоненту\n",
    "            grad[i] += dim * func_primal_grad(x, A[i], μ) # обновляем \"накопительный\" градиент только для выбранной i-ой компоненты\n",
    "        grad = grad / m # усредняем по числу выбранных компонент\n",
    "        #------------- умная реализация, в которой градиент считается один раз для всего -------------\n",
    "        # grad = np.zeros(shape=(dim, 1)) # \"накопительный\" вектор для градиента по компонентам\n",
    "        # grad_full = func_primal_grad(x, A, μ) # значение градиента для всех компонент\n",
    "        # j = np.random.randint(low=0, high=dim, size=(m)) # случайным образом вибираем m компонент для шага\n",
    "        # j = dict(zip(*np.unique(j, return_counts=True))) # конвертируем их в словарь вида \"компонента: сколько раз по ней нужно проитерироваться\"\n",
    "        # for i in j.keys(): # идём по компонентам\n",
    "        #     grad[i] += j[i] * dim * grad_full[i] # обновляем \"накопительный\" градиент только для выбранной i-ой компоненты\n",
    "        # grad = grad / m # усредняем по числу выбранных компонент\n",
    "        #=============================================================================================\n",
    "\n",
    "        x = x - η * grad # обновляем значение x (так как задача минимизации, то идём против градиента)\n",
    "        if dist(constraint_center, x) > 1: # если нарушили ограничение ~ вышли из допустимой области (Евклидово расстояние от нуля до точки должно быть <= 1)\n",
    "            x = x / dist(constraint_center, x) # берём проекцию точки на dim-мерную сферу (делим координаты на расстояние до точки, чтобы новое расстояние было равно 1)\n",
    "\n",
    "        res_stoh_grad_primal = func_primal(x, A, μ) # считаем значение функции\n",
    "\n",
    "        η = max(η * 0.99, 0.001) # слегка уменьшаем шаг, но не меньше 0.001\n",
    "        iterations += 1 # увеличиваем общее число итераций на рассматриваемой размерности dim\n",
    "        \n",
    "    return [res_stoh_grad_primal, x, iterations] # возвращаем [оптимальное значение функции, оптимальная точка x, число итераций]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in tqdm(n): # идём по возможному числу переменных (размерности пространства)\n",
    "    for m in set([1, int(dim/8), int(dim/4), int(dim/2), dim]): # рассматриваем возможные значения параметра m (int — так как m должен быть целым числом) для стохастического градиента\n",
    "        iterations = 0 # всего итераций для решения всех тест-кейсов при всех начальных точках\n",
    "        time_start = time.time() # замеряем время старта рассмотрения размерности dim\n",
    "\n",
    "        for i in range(N): # идём по числу тест-кейсов\n",
    "            A = data[dim][i][\"A\"] # матрица А для тест-кейса\n",
    "            res_solver = data[dim][i][\"Result solver\"] # результат от солвера для тест-кейса\n",
    "\n",
    "            for p in range(P): # идём по числу случайных стартовых точек\n",
    "                # генерируем начальную точку при ограничение задачи — квадрат евклидовой нормы вектора x ≤ 1\n",
    "                x = np.random.randint(low=0, high=100, size=(dim, 1)) # генерируем случайные числа размера количества переменных (изначально они могут нарушать ограничения)\n",
    "                x = x/np.linalg.norm(x, ord=2) # делим значение сгенерированного вектора на его евклидову норму, чтобы новая норма была точно ≤ 1 (на самом деле она будет лишь слегка меньше 1, но такая точка уже не нарушает ограничение задачи)\n",
    "\n",
    "                iterations += stochastic_gradient_descent_primal(x, A, res_solver, ε, m)[2] # запоминаем число итераций, что потребовалось градиентному спуску чтобы сойтись с ответом солвера с точностью ε\n",
    "        \n",
    "        data[dim][f\"Average time stochastic grad prime with m={m}\"] = (time.time() - time_start) / (N * P) # среднее время для размерности dim за (N * p) решённых вариантов задачи\n",
    "        data[dim][f\"Average iterations stochastic grad prime with m={m}\"] = iterations / (N * P) # среднее число итерации для размерности dim за (N * p) решённых вариантов задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация стохастического градиентного спуска для сильно выпуклых и гладких функций (Gradient descent for α-strongly convex and β-smooth functions with projection) в двойственной задаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для двойственной задачи, из-за наличия всего одной переменной, стохастический градиентный спуск сводится к обычному ($n \\to 1$, $m \\to 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_dual(λ: np.array, A: np.array, res_solver: np.float32, ε: np.float32, m: np.float32) -> list:\n",
    "    \"\"\"\n",
    "    Метод стохастического градиентного спуска для подсчёта оптимума двойственной задачи.\\n\n",
    "    Parameters:\n",
    "        * λ: изначальное значения λ\n",
    "        * A: матрица A\n",
    "        * res_solver: уже полученный ответ от солвера, к которому нужно сойтись\n",
    "        * ε: необходимая точность ответа\n",
    "        * m: \\n\n",
    "    Returns:\n",
    "        * list: [оптимальное значение функции, оптимальная точка x, число итераций]\n",
    "    \"\"\"\n",
    "    iterations = 0 # счётчик итераций стохастического градиентного спуска\n",
    "\n",
    "    # считаем шаг для метода\n",
    "    eigenvalues = np.linalg.eigvals(A) # собственные значения матрицы A\n",
    "    α = 2 * eigenvalues.min() # параметр выпуклости функции, в данном случае равен 2 * минимальное собственное значение матрицы A\n",
    "    β = 2 * eigenvalues.max() # параметр гладкости функции, в данном случае равен 2 * максимальное собственное значение матрицы A\n",
    "    η = 2 / (α + β) # значение шага\n",
    "\n",
    "    dim = λ.shape[0] # размерность пространства\n",
    "    μ = np.ones(shape=(A.shape[0], 1)) # смещение центра функции, (A.shape[0], 1) — в виде столбца, по размеру должен совпадать с прямой задачей\n",
    "\n",
    "    res_stoh_grad_dual = func_dual(λ, A, μ) # значение начального решения для рассматриваемой стартовой точки\n",
    "    while abs(res_solver - res_stoh_grad_dual) > ε: # пока не сошлись с ответом солвера\n",
    "        grad = np.zeros(shape=(dim, 1)) # \"накопительный\" вектор для градиента по компонентам\n",
    "        grad_full = func_dual_grad(λ, A, μ) # значение градиента для всех компонент (в двойственной лишь одна компонента, поэтому это значение можно запомнить чтобы не производить перещёт градиента m раз)\n",
    "        for j in range(m): # делаем m выборов компоненты, по которой будет делать градиентный шаг\n",
    "            i = np.random.randint(low=0, high=dim, size=(1)) # случайным образом выбираем компоненту\n",
    "            grad[i] += dim * grad_full # обновляем \"накопительный\" градиент только для выбранной i-ой компоненты\n",
    "        grad = grad / m # усредняем по числу выбранных компонент\n",
    "\n",
    "        λ = λ + η * grad # обновляем значение λ (так как задача максимизации, то идём в сторону градиента)\n",
    "        if not constraints_dual(λ): # если нарушено ограничение на λ\n",
    "            λ = 0\n",
    "\n",
    "        res_stoh_grad_dual = func_dual(λ, A, μ) # считаем значение функции\n",
    "\n",
    "        η = max(η * 0.99, 0.001) # слегка уменьшаем шаг, но не меньше 0.001\n",
    "        iterations += 1 # увеличиваем общее число итераций на рассматриваемой размерности dim\n",
    "        \n",
    "    return [res_stoh_grad_dual, x, iterations] # возвращаем [оптимальное значение функции, оптимальная точка λ, число итераций]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in tqdm(n): # идём по возможному числу переменных (размерности пространства)\n",
    "    for m in set([1, int(dim/8), int(dim/4), int(dim/2), dim]): # рассматриваем возможные значения параметра m (int — так как m должен быть целым числом) для стохастического градиента\n",
    "        iterations = 0 # всего итераций для решения всех тест-кейсов при всех начальных точках\n",
    "        time_start = time.time() # замеряем время старта рассмотрения размерности dim\n",
    "\n",
    "        for i in range(N): # идём по числу тест-кейсов\n",
    "            A = data[dim][i][\"A\"] # матрица А для тест-кейса\n",
    "            res_solver = data[dim][i][\"Result solver\"] # результат от солвера для тест-кейса\n",
    "            \n",
    "            for p in tqdm(range(P)): # идём по числу случайных стартовых точек\n",
    "                λ = np.random.rand(1, 1) # генерируем случайное значение λ (np.array двойной вложенности) из равномерного распределения [0, 1), удоавлетворяющее ограничению λ ≥ 0\n",
    "                \n",
    "                iterations += stochastic_gradient_descent_dual(λ, A, res_solver, ε, m)[2] # запоминаем число итераций, что потребовалось градиентному спуску чтобы сойтись с ответом солвера с точностью ε\n",
    "\n",
    "        data[dim][f\"Average time stochastic grad dual with m={m}\"] = (time.time() - time_start) / (N * P) # среднее время для размерности dim за (N * p) решённых вариантов задачи\n",
    "        data[dim][f\"Average iterations stochastic grad dual with m={m}\"] = iterations / (N * P) # среднее число итерации для размерности dim за (N * p) решённых вариантов задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Сравнение результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для заданной точности $ε > 0$ **по значению целевой функции** и для каждого значения размерности $n ∈ \\{10, 20, ..., 100\\}$ и $m ∈ \\{1, \\frac{n}{8} , \\frac{n}{4} , \\frac{n}{2} , n\\}$ подсчитайте ***среднее число арифметических операций*** для обычного и стохастического градиентного спуска в прямой задаче и в двойственной (стохастический метод для двойственной задачи в данном случае аналогичен прямому методу), усреднение проводится по всем начальным точкам P и по всем тестовым примерам N. Если число операций подсчитать не удается, то указать среднее время работы метода. Какой метод работает лучше и почему?\n",
    "* Для одного тестового примера при $n = 10$ и нескольких различных начальных точек постройте ***зависимость точности от числа арифметических операций*** в методах градиентного и стохастического градиентного спуска, примененных для прямой задачи. Если число операций подсчитать не получается, то построить зависимость средней точности от времени работы метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cplex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
